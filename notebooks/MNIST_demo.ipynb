{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: this notebook is merely an example of usage of the `layer_sim` library and doesn't focus on the performance of the provided NNs nor on the accurate analysis of the resulting similarities.\n",
    "The code provided is run on MNIST so that everyone may reproduce the results in a small enough amount of time on a medium-sized machine without a CUDA-capable GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"..\") # so we can import the layer_sim library\n",
    "\n",
    "from layer_sim import networks\n",
    "from layer_sim import datasets\n",
    "from layer_sim import nn_comparison\n",
    "from layer_sim import preprocessing\n",
    "from layer_sim.train import train_net, test_net\n",
    "from layer_sim.pruning.IMP import imp_lrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and NN preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_batch = 128\n",
    "test_batch = 128\n",
    "trainloader, testloader = datasets.MNIST(\"../data\", train_batch, test_batch, num_workers=4)\n",
    "net = networks.LeNet5(num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: insert image of LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_init = 0.1\n",
    "weight_decay = 0.0001\n",
    "momentum = 0.9\n",
    "epochs = 15\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_annealing_rate = 10\n",
    "lr_annealing_schedule = [10, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr_init, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "===> Epoch 1/15 ### Loss 0.3238158259967963 ### Performance 0.8953333333333333\n===> Epoch 2/15 ### Loss 0.06931836195240418 ### Performance 0.9788333333333333\n===> Epoch 3/15 ### Loss 0.04757271823982398 ### Performance 0.9855833333333334\n===> Epoch 4/15 ### Loss 0.04082236483022571 ### Performance 0.9876833333333334\n===> Epoch 5/15 ### Loss 0.032276718960702416 ### Performance 0.9903\n===> Epoch 6/15 ### Loss 0.028784222680702805 ### Performance 0.9910666666666667\n===> Epoch 7/15 ### Loss 0.02341511012427509 ### Performance 0.9927666666666667\n===> Epoch 8/15 ### Loss 0.024698692759002248 ### Performance 0.9923833333333333\n===> Epoch 9/15 ### Loss 0.023087299082769703 ### Performance 0.9929666666666667\n===> Epoch 10/15 ### Loss 0.020946485105218987 ### Performance 0.9932166666666666\n===> Epoch 11/15 ### Loss 0.016838115830874693 ### Performance 0.9945833333333334\nLR annealed: previous 0.1, current 0.01\n===> Epoch 12/15 ### Loss 0.007709789935398536 ### Performance 0.9976\n===> Epoch 13/15 ### Loss 0.004453768027938592 ### Performance 0.99885\nLR annealed: previous 0.01, current 0.001\n===> Epoch 14/15 ### Loss 0.0035638416282987844 ### Performance 0.99915\n===> Epoch 15/15 ### Loss 0.003456470188164773 ### Performance 0.9992166666666666\n"
    }
   ],
   "source": [
    "tr_loss, tr_perf = train_net(net, epochs, criterion, optimizer, trainloader, device=device, lr_annealing_factor=lr_annealing_rate, epochs_annealing=lr_annealing_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "===> TEST ### Loss 0.02399575710296631 ### Performance 0.9939\n"
    }
   ],
   "source": [
    "te_loss, te_perf = test_net(net, testloader, criterion, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: save the NN\n",
    "\n",
    "In the next cell we save the state_dict along with a number of auxiliary data (train/test loss/performance) in a dictionary called `save_dict`. We save this dict in a `save_root` which we will use also as a base for the IMP checkpoints.\n",
    "\n",
    "The `save_dict` mimics the structure of IMP's checkpoint (minus the pruning mask, which is absent in the case of the complete model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict = {\n",
    "    \"train_loss\": tr_loss,\n",
    "    \"train_perf\": tr_perf,\n",
    "    \"test_loss\": te_loss,\n",
    "    \"test_perf\": te_perf,\n",
    "    \"parameters\": net.state_dict()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root = \"../models/LeNet5\"\n",
    "save_name = \"complete_net.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(save_dict, os.path.join(save_root, save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# load the NN\n",
    "net.load_state_dict(torch.load(os.path.join(save_root, save_name))[\"parameters\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store models representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloader for representation w/ Train set as False\n",
    "reprloader, _ = datasets.MNIST(\"../data\", 128, train=False, num_workers=4)\n",
    "datapoints_repr = 500\n",
    "layers_to_hook = (torch.nn.ReLU, torch.nn.AvgPool2d)\n",
    "compl_repr = net.extract_network_representation(reprloader, limit_datapoints=datapoints_repr, layer_types_to_hook=layers_to_hook, device=\"cpu\", retain_grad=True, loss_fn=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SVCCA\n",
    "SVCCA_ROOT = \"../../svcca\"\n",
    "sys.path.append(os.path.expanduser(SVCCA_ROOT))\n",
    "from cca_core import get_cca_similarity\n",
    "\n",
    "# prepare lambda fct to get scalar for mean_cca_similarity\n",
    "mean_cca_sim = lambda x,y: get_cca_similarity(x,y)[\"mean\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get kernels of representations (for CKA & NBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define fct to get linear kernels\n",
    "# linear kernel is just M M^T, where M is a matrix whose rows are datapoints and columns are the neurons\n",
    "def get_linear_kernel(matrix, grad=False):\n",
    "    # if matrix is more than two-dimensional, flatten the last dimensions into a single one\n",
    "    if len(matrix.shape) == 4:\n",
    "        matrix_2d = preprocessing.reshape_4d_tensor(matrix)\n",
    "        ker = matrix_2d @ matrix_2d.T\n",
    "        if grad:\n",
    "            kgrad = matrix_2d.grad @ matrix_2d.grad.T\n",
    "            return ker * kgrad\n",
    "        return ker\n",
    "    ker = matrix @ matrix.T\n",
    "    if grad:\n",
    "        kgrad = matrix.grad @ matrix.grad.T\n",
    "        return ker * kgrad\n",
    "    return ker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels_compl1 = [get_linear_kernel(r) for r in compl_repr1]\n",
    "kernels_compl2 = [get_linear_kernel(r) for r in compl_repr2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline_cca(tensor, var_kept = .99):\n",
    "    if len(tensor.shape) == 4:\n",
    "        tensor = preprocessing.reshape_4d_tensor(tensor, True)\n",
    "    tensor = preprocessing.svd_reduction(tensor, var_kept)\n",
    "    return tensor.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "compl_repr1 = [preprocess_pipeline_cca(r) for r in compl_repr1]\n",
    "compl_repr2 = [preprocess_pipeline_cca(r) for r in compl_repr2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "adding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\n"
    }
   ],
   "source": [
    "# store measurements in tensor whose dimensions are: metric, iteration, layer\n",
    "similarities = torch.zeros([2, len(compl_repr)])\n",
    "\n",
    "for l, (layer1, layer2) in enumerate(zip(compl_repr1, compl_repr2)):\n",
    "    similarities[0, l] = mean_cca_sim(compl.detach().numpy(), pruned.detach().numpy())\n",
    "for l, (layer1, layer2) in enumerate(zip(kernels_compl1, kernels_compl2)):\n",
    "    similarities[1, l] = nn_comparison.cka(layer1.detach(), layer2.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[0.9997, 0.9998, 0.9985, 0.9990, 0.9888, 0.9858, 0.9992],\n         [0.9980, 0.9985, 0.9889, 0.9914, 0.9331, 0.9302, 0.9944]],\n\n        [[0.4692, 0.3582, 0.7535, 0.6547, 0.8563, 0.7598, 0.9681],\n         [0.4725, 0.3637, 0.7509, 0.6518, 0.8333, 0.7490, 0.9577]]])"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "similarities"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1593706732852",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}