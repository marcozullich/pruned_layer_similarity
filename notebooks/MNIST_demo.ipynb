{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: this notebook is merely an example of usage of the `layer_sim` library and doesn't focus on the performance of the provided NNs nor on the accurate analysis of the resulting similarities.\n",
    "The code provided is run on MNIST so that everyone may reproduce the results in a small enough amount of time on a medium-sized machine without a CUDA-capable GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"..\") # so we can import the layer_sim library\n",
    "\n",
    "from layer_sim import networks\n",
    "from layer_sim import datasets\n",
    "from layer_sim import nn_comparison\n",
    "from layer_sim import preprocessing\n",
    "from layer_sim.train import train_net, test_net\n",
    "from layer_sim.pruning.IMP import imp_lrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_batch = 128\n",
    "test_batch = 128\n",
    "trainloader, testloader = datasets.MNIST(\"../data\", train_batch, test_batch, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_init = 0.1\n",
    "weight_decay = 0.0001\n",
    "momentum = 0.9\n",
    "epochs = 15\n",
    "device = \"cpu\"\n",
    "\n",
    "reps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "===> Epoch 1/15 ### Loss 0.3238158259967963 ### Performance 0.8953333333333333\n===> Epoch 2/15 ### Loss 0.06931836195240418 ### Performance 0.9788333333333333\n===> Epoch 3/15 ### Loss 0.04757271823982398 ### Performance 0.9855833333333334\n===> Epoch 4/15 ### Loss 0.04082236483022571 ### Performance 0.9876833333333334\n===> Epoch 5/15 ### Loss 0.032276718960702416 ### Performance 0.9903\n===> Epoch 6/15 ### Loss 0.028784222680702805 ### Performance 0.9910666666666667\n===> Epoch 7/15 ### Loss 0.02341511012427509 ### Performance 0.9927666666666667\n===> Epoch 8/15 ### Loss 0.024698692759002248 ### Performance 0.9923833333333333\n===> Epoch 9/15 ### Loss 0.023087299082769703 ### Performance 0.9929666666666667\n===> Epoch 10/15 ### Loss 0.020946485105218987 ### Performance 0.9932166666666666\n===> Epoch 11/15 ### Loss 0.016838115830874693 ### Performance 0.9945833333333334\nLR annealed: previous 0.1, current 0.01\n===> Epoch 12/15 ### Loss 0.007709789935398536 ### Performance 0.9976\n===> Epoch 13/15 ### Loss 0.004453768027938592 ### Performance 0.99885\nLR annealed: previous 0.01, current 0.001\n===> Epoch 14/15 ### Loss 0.0035638416282987844 ### Performance 0.99915\n===> Epoch 15/15 ### Loss 0.003456470188164773 ### Performance 0.9992166666666666\n"
    }
   ],
   "source": [
    "save_root = \"../models/LeNet5\"\n",
    "save_name = \"trained_net_{}.pt\"\n",
    "for i in range(reps):\n",
    "    net = networks.LeNet5(num_classes=10)\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    tr_loss, tr_perf = train_net(net, epochs, criterion, optimizer, trainloader, device=device)\n",
    "    te_loss, te_perf = test_net(net, testloader, criterion, device)\n",
    "    save_dict = {\n",
    "        \"train_loss\": tr_loss,\n",
    "        \"train_perf\": tr_perf,\n",
    "        \"test_loss\": te_loss,\n",
    "        \"test_perf\": te_perf,\n",
    "        \"parameters\": net.state_dict()\n",
    "    }\n",
    "    torch.save(save_dict, os.path.join(save_root, save_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store complete model's representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloader for representation w/ Train set as False\n",
    "reprloader, _ = datasets.MNIST(\"../data\", 128, train=False, num_workers=4)\n",
    "datapoints_repr = 500\n",
    "layers_to_hook = (torch.nn.ReLU, torch.nn.AvgPool2d)\n",
    "\n",
    "net = networks.LeNet5(num_classes=10)\n",
    "net.load_state_dict(torch.load(\"../models/LeNet5/trained_net_0.pt\")[\"parameters\"])\n",
    "net_repr1 = net.extract_network_representation(reprloader, limit_datapoints=datapoints_repr, layer_types_to_hook=layers_to_hook, device=\"cpu\")\n",
    "net.load_state_dict(torch.load(\"../models/LeNet5/trained_net_1.pt\")[\"parameters\"])\n",
    "net_repr2 = net.extract_network_representation(reprloader, limit_datapoints=datapoints_repr, layer_types_to_hook=layers_to_hook, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SVCCA\n",
    "SVCCA_ROOT = \"C:\\\\Users\\\\mzullich\\\\Documents\\\\svcca\"\n",
    "sys.path.append(os.path.expanduser(SVCCA_ROOT))\n",
    "from cca_core import get_cca_similarity\n",
    "\n",
    "# prepare lambda fct to get scalar for mean_cca_similarity\n",
    "mean_cca_sim = lambda x,y: get_cca_similarity(x,y)[\"mean\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define fct to get linear kernels\n",
    "# linear kernel is just M M^T, where M is a matrix whose rows are datapoints and columns are the neurons\n",
    "def get_linear_kernel(matrix):\n",
    "    # if matrix is more than two-dimensional, flatten the last dimensions into a single one\n",
    "    if len(matrix.shape) == 4:\n",
    "        matrix_2d = preprocessing.reshape_4d_tensor(matrix)\n",
    "        return matrix_2d @ matrix_2d.T\n",
    "    return matrix @ matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels_1 = [get_linear_kernel(r) for r in net_repr1]\n",
    "kernels_2 = [get_linear_kernel(r) for r in net_repr2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([500, 500])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "kernels_1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline_cca(tensor, var_kept = .99):\n",
    "    if len(tensor.shape) == 4:\n",
    "        tensor = preprocessing.reshape_4d_tensor(tensor, True)\n",
    "    tensor = preprocessing.svd_reduction(tensor, var_kept)\n",
    "    return tensor.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_repr1 = [preprocess_pipeline_cca(r) for r in net_repr1]\n",
    "preprocessed_repr2 = [preprocess_pipeline_cca(r) for r in net_repr2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "adding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\nadding eps to diagonal and taking inverse\ntaking square root\ndot products...\ntrying to take final svd\ncomputed everything!\n"
     ]
    }
   ],
   "source": [
    "# store measurements in tensor whose dimensions are: metric, iteration, layer\n",
    "similarities = torch.zeros([2, len(preprocessed_repr1)])\n",
    "\n",
    "for l, (r1, r2) in enumerate(zip(preprocessed_repr1, preprocessed_repr2)):\n",
    "    similarities[0, l] = mean_cca_sim(r1.detach().numpy(), r2.detach().numpy())\n",
    "for l, (r1, r2) in enumerate(zip(kernels_1, kernels_2)):\n",
    "    similarities[1, l] = nn_comparison.cka(r1.detach(), r2.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.7036, 0.7124, 0.6040, 0.7061, 0.7852, 0.7711, 0.9486],\n",
       "        [0.9557, 0.9576, 0.9430, 0.9324, 0.9384, 0.9489, 0.9330]])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "similarities"
   ]
  },
  {
   "source": [
    "### Homework\n",
    "\n",
    "_NB: (§§§) indicates a hard exercise, (§§) a moderately hard exercise_\n",
    "\n",
    "Reproduce the \"Sanity Check for Similarity Indexes\" from page 6 of [Similarity of Neural Network Representations Revisited, Kornblith et al.](https://arxiv.org/abs/1905.00414) for the case of Multilayer Perceptrons (MLPs).\n",
    "\n",
    "1. Start from a MLP with an architecture of your choice. \n",
    "\n",
    "  a. _(extra 1) The architecture must be such that it reaches 98% of test-set accuracy on average_   \n",
    "    * _Test with an appropriate statistic that this threshold is reached_\n",
    "        \n",
    "2. (§§) Build a function to extract representations from each layer *after* the application of its activation function\n",
    "3. Operate a pairwise layer comparison **for each layer in the architecture** at least for 2 parameters sets\n",
    "    a. Use both CKA and SVCCA\n",
    "4. (§§§) _(extra 2) Fix the `layer_sim` library such that it is possible to retain the gradient of the representations (you'll need to call `backward` inside the routine for building representations. You can do it either on the loss or check [Similarity of Neural Networks with Gradients](https://arxiv.org/abs/2003.11498) for additional tricks) and implement CKA with the incorporation of gradient flow._\n",
    "\n",
    "    a. _See how this metric compares to *vanilla* CKA_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "7bd19c094dc44574d64e00d65102e6ba36e1076719a93266f721ddb81f822a22"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}