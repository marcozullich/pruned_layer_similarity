{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\") # so we can import the layer_sim library\n",
    "\n",
    "from layer_sim import networks\n",
    "from layer_sim import datasets\n",
    "from layer_sim.train import train_net, test_net\n",
    "from layer_sim.pruning.IMP import imp_lrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and NN preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_batch = 128\n",
    "test_batch = 128\n",
    "trainloader, testloader = datasets.MNIST(\"../data\", train_batch, test_batch, num_workers=4)\n",
    "net = networks.LeNet5(num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: insert image of LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_init = 0.1\n",
    "weight_decay = 0.0001\n",
    "momentum = 0.9\n",
    "epochs = 15\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_annealing_rate = 10\n",
    "lr_annealing_schedule = [10, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "===> Epoch 1/15 ### Loss 0.32814517916639646 ### Performance 0.8958666666666667\n===> Epoch 2/15 ### Loss 0.07053277128438155 ### Performance 0.9790166666666666\n===> Epoch 3/15 ### Loss 0.048630800982316334 ### Performance 0.9855166666666667\n===> Epoch 4/15 ### Loss 0.04039812043358882 ### Performance 0.98765\n===> Epoch 5/15 ### Loss 0.03454992857140799 ### Performance 0.9894166666666667\n===> Epoch 6/15 ### Loss 0.03000268583421906 ### Performance 0.9911\n===> Epoch 7/15 ### Loss 0.02701919445786625 ### Performance 0.9917166666666667\n===> Epoch 8/15 ### Loss 0.02189559004077067 ### Performance 0.99315\n===> Epoch 9/15 ### Loss 0.022885738459710654 ### Performance 0.9929833333333333\n===> Epoch 10/15 ### Loss 0.01977641623740395 ### Performance 0.9937833333333334\n===> Epoch 11/15 ### Loss 0.02014222490272174 ### Performance 0.9936833333333334\nLR annealed: previous 0.1, current 0.01\n===> Epoch 12/15 ### Loss 0.007557284027454443 ### Performance 0.99775\n===> Epoch 13/15 ### Loss 0.004766303374920971 ### Performance 0.9988\nLR annealed: previous 0.01, current 0.001\n===> Epoch 14/15 ### Loss 0.00378708644915993 ### Performance 0.9991166666666667\n===> Epoch 15/15 ### Loss 0.0036606240635427335 ### Performance 0.9991166666666667\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(0.0036606240635427335, 0.9991166666666667)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr_init, momentum=momentum, weight_decay=weight_decay)\n",
    "train_net(net, epochs, criterion, optimizer, trainloader, device=device, lr_annealing_factor=lr_annealing_rate, epochs_annealing=lr_annealing_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "===> TEST ### Loss 0.027738256379961967 ### Performance 0.9924\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor(0.0277), 0.9924)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "test_net(net, testloader, criterion, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store complete model's representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloader for representation w/ Train set as False\n",
    "reprloader, _ = datasets.MNIST(\"../data\", 128, train=False, num_workers=4)\n",
    "compl = net.forward_with_hooks() # TODO: limit datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMP application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: we can help ourselves with `net.state_dict().keys()` to enucleate the layers names which we'll be pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "odict_keys(['features.0.weight', 'features.0.bias', 'features.3.weight', 'features.3.bias', 'classifier.0.weight', 'classifier.0.bias', 'classifier.2.weight', 'classifier.2.bias', 'classifier.4.weight', 'classifier.4.bias'])"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "net.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we wish to prune all of the weights & biases of the conv layers + the f-c layers minus the last one.\n",
    "The selection is operated on the keys of the state_dict compared using regex:\n",
    "\n",
    "* we can use a generic `\"features\"` pattern to catch all of the conv layers\n",
    "* we use a more specific regex `\"classifier\\.[02]\\.\"` to catch the first 2 f-c layers (but not the last one which has ID `4` in the state_dict keys)\n",
    "\n",
    "Note: if we had BatchNorm layers, we should be more careful in indicating the layers to prune since, using `torch.nn.Sequential`s inside the NN, the name of the BN parameters is usually associated to the closest conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_prune = [\"features\", r\"classifier\\.[02]\\.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=====> Iteration of IMP: 1/2\n"
    },
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 1)",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\mzullich\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3325\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-16-40e9b3b2c8a5>\"\u001b[0m, line \u001b[0;32m3\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    imp_lrr(net, epochs, criterion, optimizer, trainloader, imp_iterations, device=device, testloader=testloader, save_path=save_path, layer_ids_to_prune=layers_to_prune)\n",
      "  File \u001b[0;32m\"..\\layer_sim\\pruning\\IMP.py\"\u001b[0m, line \u001b[0;32m57\u001b[0m, in \u001b[0;35mimp_lrr\u001b[0m\n    train_loss, train_perf = train_net(net, epochs, criterion, optimizer, trainloader, device, performance, mask=mask, **kwargs)\n",
      "  File \u001b[0;32m\"..\\layer_sim\\train.py\"\u001b[0m, line \u001b[0;32m96\u001b[0m, in \u001b[0;35mtrain_net\u001b[0m\n    apply_mask(net, mask, gradient=True)\n",
      "\u001b[1;36m  File \u001b[1;32m\"..\\layer_sim\\pruning\\LF_mask.py\"\u001b[1;36m, line \u001b[1;32m95\u001b[1;36m, in \u001b[1;35mapply_mask\u001b[1;36m\u001b[0m\n\u001b[1;33m    exec(\"model.%s.grad *= m.to(device)\" % name)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"<string>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    model.features.0.weight.grad *= m.to(device)\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "imp_iterations = 2\n",
    "save_path=\"../models/LeNet5/IMP\" # checkpoints will be saved in this folder as IMP_checkpoint_n.pt, where `n` is the iteration number\n",
    "imp_lrr(net, epochs, criterion, optimizer, trainloader, imp_iterations, device=device, testloader=testloader, save_path=save_path, layer_ids_to_prune=layers_to_prune)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1593523976086",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}